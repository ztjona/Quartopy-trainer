# E04b N_LAST_STATES Experiment Analysis - Summary

**Analysis Date:** November 20, 2025  
**Experiment:** E04b (Testing Curriculum Learning Schedules - 1000 epochs)  
**Notebook:** `01_E04_comprehensive_analysis.ipynb`

## Executive Summary

This analysis evaluated different **curriculum learning schedules** for Quarto bot training over 1000 epochs. The experiments test how well the agent adapts when game history length gradually increases during training.

### üéì Curriculum Learning Design

**ALL experiments use curriculum learning** where `n_last_states` starts at 2 and linearly interpolates to `N_LAST_STATES_FINAL` over 1000 epochs:

| Experiment | Curriculum Schedule | Interpretation |
|-----------|-------------------|----------------|
| E04b_FINAL2 | 2 ‚Üí 2 | **No curriculum** (constant baseline) |
| E04b_FINAL4 | 2 ‚Üí 4 | Mild curriculum (2x complexity) |
| E04b_FINAL8 | 2 ‚Üí 8 | Moderate curriculum (4x complexity) |
| E04b_FINAL12 | 2 ‚Üí 12 | Strong curriculum (6x complexity) |
| E04b_FINAL16 | 2 ‚Üí 16 | Maximum curriculum (8x, full game) |

**Pedagogical Goal:** Agent learns final position tactics first (n=2), then gradually sees more game history to develop strategic planning.

### Experiments Analyzed

- **E04b variants**: Full 1000-epoch training runs with curriculum learning
- **Total configurations**: 5 curriculum schedules (2‚Üí2, 2‚Üí4, 2‚Üí8, 2‚Üí12, 2‚Üí16)
- **Note**: E04 (100-epoch runs) excluded as incomplete training

## Key Findings

### 1. Performance Metrics (E04b - 1000 epochs)

**Final Performance (Last 10 Epochs Average):**

| N_LAST_STATES | vs bot_good_WR_B | vs bot_random | Final Loss | Loss Stability |
|---------------|------------------|---------------|------------|----------------|
| 2             | 0.375 ¬± 0.095   | 0.542 ¬± 0.086 | 0.026      | ‚úì STABLE       |
| 4             | 0.353 ¬± 0.101   | 0.470 ¬± 0.111 | 0.314      | ~ MODERATE     |
| 8             | 0.333 ¬± 0.077   | 0.510 ¬± 0.090 | 0.428      | ‚úó UNSTABLE     |
| 12            | 0.370 ¬± 0.087   | 0.490 ¬± 0.094 | 0.458      | ‚úó UNSTABLE     |
| 16            | 0.335 ¬± 0.082   | 0.505 ¬± 0.085 | 0.458      | ‚úó UNSTABLE     |

### 2. üîç Critical Discovery: Curriculum Learning Reveals Architectural Limitations

**Most Important Finding - Loss Divergence Pattern:**
- **N=2 (no curriculum)**: Loss remains **stable** (~0.026) - task complexity is constant ‚úì
- **N=4 (mild curriculum)**: Loss increases moderately (14x) - network mostly adapts ‚ñ≥
- **N=8/12/16 (aggressive curriculum)**: Loss **diverges** (15-17x) - curriculum too fast ‚úó

**Why does loss increase for curriculum schedules?**

The loss divergence is **NOT a bug** - it reveals that the curriculum changes task complexity faster than the network can adapt:

1. **Architecture Mismatch**: Network designed for short, fixed-length inputs (n=2) struggles with growing variable-length sequences
2. **Distribution Shift**: As n increases from 2‚Üí16, the input distribution changes dramatically (short sequences ‚Üí long sequences)
3. **Replay Buffer Conflict**: Mixing experiences from different curriculum stages (simple + complex) creates non-stationary learning problem
4. **Capacity Limits**: Current architecture lacks capacity for variable-length sequence adaptation
5. **Schedule Too Aggressive**: 1000 epochs insufficient to adapt from n=2 to n=16 (8x complexity increase)

**Curriculum learning is pedagogically sound, but the implementation reveals the network needs architectural support (e.g., RNNs, Transformers, attention) to handle variable-length sequences.**

**This makes N=2 the CLEAR winner despite similar win rates!**

### 3. Statistical Analysis

- **ANOVA Results:** No statistically significant difference in win rates (p > 0.05)
- **However**: Loss stability shows CLEAR differences - N=2 vastly superior

### 4. Learning Speed

All variants reached performance thresholds very quickly:
- **30-50% win rate:** Immediately (epoch 0)
- **60% win rate:** By epoch 10
- **70% win rate:** Only N=16 reached this milestone (epoch 147)

Against bot_random, all variants achieved high performance (>60%) from the start.

### 5. Training Dynamics

**Observations (E04b - 1000 epochs):**
- All variants show **negative improvement** (performance decline) over training
- All show relatively flat, stable trends (minimal epoch-to-epoch fluctuation)
- Lower variance compared to shorter training runs
- **Consistent degradation across all N_LAST_STATES values**

## üèÜ Recommendation

### **STRONG Recommendation: Constant N_LAST_STATES = 2 (No Curriculum)**

**Rationale:**
- **Loss Stability:** ‚úì ONLY configuration with stable training (no curriculum = no distribution shift)
- **Combined Score:** 0.5008 (highest win rate performance)
- **Average Performance:** 0.375 vs bot_good_WR_B (best)
- **Final Loss:** 0.026 (vs 0.31-0.46 for curriculum variants)
- **Training Stability:** No divergence, predictable behavior
- **Computational Efficiency:** Lower overhead processing only 2 states

**Why NOT curriculum learning (N>2):**
- **N=4**: Moderate loss increase (14x), network shows strain adapting to curriculum
- **N‚â•8**: **Severe loss divergence** (15-17√ó), curriculum too aggressive for architecture
- Current network architecture not designed for variable-length sequences
- Replay buffer mixing curriculum stages hurts learning
- No performance benefit to justify training instability

## Conclusions About Curriculum Learning

### What We Learned:

**The pedagogical intuition was correct** (start simple with final positions, gradually increase complexity), **but the implementation revealed architectural constraints:**

1. ‚úì **Idea is sound**: Learning final positions first (n=2) then building to full game history (n=16) makes pedagogical sense
2. ‚úó **Current architecture inadequate**: Network designed for fixed-length inputs struggles with growing sequences
3. ‚úó **Schedule too aggressive**: 1000 epochs insufficient for 8x complexity increase (2‚Üí16)
4. ‚úó **Replay buffer incompatible**: Mixing experiences from different curriculum stages creates non-stationarity

### Future Directions:

**Option A: Use Constant N=2 (RECOMMENDED FOR NOW)**
- Proven stable and effective for learning final position tactics
- Focus improvements elsewhere (opponent diversity, early stopping, etc.)

**Option B: Implement Proper Curriculum Learning (RESEARCH PROJECT)**
Requirements to make curriculum learning work:
- **Architecture**: Design for variable-length sequences (RNNs, Transformers, attention mechanisms)
- **Schedule**: Slower curriculum (5000+ epochs) or adaptive based on loss stability
- **Replay Buffer**: Curriculum-aware (separate buffers per phase, or weighted sampling)
- **Progressive Network**: Grow network capacity as task complexity increases
- **Monitoring**: Early stopping when curriculum stage causes instability

## Important Observations

### Performance Degradation Over Time

**Critical Finding:** All E04b experiments (1000 epochs) show performance **degradation**:
- Early epochs (1-10): ~47.5% vs bot_good_WR_B, ~65% vs bot_random
- Late epochs (990-1000): ~33-37.5% vs bot_good_WR_B, ~47-54% vs bot_random
- **All N_LAST_STATES values exhibit this pattern**

**Possible Causes:**
1. **Overfitting:** Models overfit to self-play patterns
2. **Catastrophic Forgetting:** Extended training causes models to forget earlier effective strategies
3. **Training Instability:** Self-play without diverse opponents leads to strategy collapse
4. **Exploration-Exploitation Balance:** Temperature settings may not be optimal for long training

### Visualization Note

**All plots now display embedded in the notebook** using Plotly's notebook renderer instead of opening in browser.

## Recommendations for Future Work

1. **Implement Early Stopping:** Save best checkpoints based on win rate peaks
2. **Curriculum Learning:** Gradually increase opponent difficulty
3. **Population-Based Training:** Maintain diverse population of bots
4. **Regularization:** Add techniques to prevent overfitting to self-play
5. **Opponent Pool:** Include external baselines in training loop
6. **Hyperparameter Tuning:**
   - Adjust temperature schedules
   - Modify replay buffer size
   - Tune learning rate decay

## Conclusion

While **N_LAST_STATES = 2** is recommended based on the comprehensive scoring, the **lack of significant statistical difference** suggests that:

1. The choice of N_LAST_STATES has **minimal impact** on final performance
2. Other factors (training stability, opponent diversity) are **more critical**
3. Focus should shift to addressing the **performance degradation** issue

The analysis reveals that the current training approach has fundamental issues with long-term stability and continuous improvement, which should be addressed before fine-tuning N_LAST_STATES.

## Files Generated

- `01_E04_comprehensive_analysis.ipynb`: Complete analysis notebook with visualizations
- `E04_ANALYSIS_SUMMARY.md`: This summary document

## Next Steps

1. ‚úÖ Review analysis results
2. ‚è≠Ô∏è Implement early stopping mechanism
3. ‚è≠Ô∏è Run ablation studies on training stability
4. ‚è≠Ô∏è Test population-based training approach
5. ‚è≠Ô∏è Evaluate best checkpoints in tournament settings
